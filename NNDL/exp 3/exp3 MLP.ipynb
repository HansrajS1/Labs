{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hansr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8642 - loss: 0.4447\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9664 - loss: 0.1079\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.9778 - loss: 0.0677\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9827 - loss: 0.0513\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9855 - loss: 0.0437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18bdcc58830>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=128, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9767 - loss: 0.0800\n",
      "Test accuracy: 0.9793000221252441\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights: [[ 0.03325083 -0.06214788  0.03930541 ...  0.03621905 -0.05933304\n",
      "   0.01164348]\n",
      " [ 0.05260804  0.06212325  0.0353731  ... -0.05294704  0.05622197\n",
      "   0.0529006 ]\n",
      " [-0.05792185  0.05575044  0.03147085 ...  0.01082849  0.04743164\n",
      "  -0.03275929]\n",
      " ...\n",
      " [ 0.05601173 -0.02390706 -0.04790973 ...  0.02712364 -0.06760988\n",
      "  -0.01749307]\n",
      " [ 0.03868255 -0.01754171  0.03909523 ... -0.05874987  0.03323543\n",
      "   0.01893892]\n",
      " [ 0.05305529  0.04722195 -0.06341346 ... -0.02583286  0.05360557\n",
      "  -0.01695624]]\n",
      "Final bias: [ 0.0045886   0.00961034  0.02340521  0.03650933  0.00352462 -0.0328263\n",
      "  0.02473915 -0.02692211  0.00296811  0.02932707 -0.03881703  0.00089725\n",
      " -0.03678086 -0.01496522 -0.02883534  0.01683288 -0.0271311   0.03355413\n",
      "  0.01260505  0.01753343  0.00859414  0.00394835 -0.00934508 -0.03381809\n",
      "  0.00145735 -0.05161307 -0.04947655  0.02749769 -0.03398044 -0.03901347\n",
      "  0.0012822  -0.02649343 -0.04760164 -0.00013125 -0.02471284 -0.04240742\n",
      "  0.00894862  0.0194146   0.0630371  -0.01060266  0.03636453 -0.00532343\n",
      " -0.02250922 -0.01598149  0.02025138  0.03368852 -0.00910943  0.02599097\n",
      " -0.0074701  -0.02222389  0.00229762 -0.02362487 -0.02963435 -0.02431241\n",
      " -0.03453721  0.00340791  0.03875246 -0.01941972 -0.02832362 -0.00674687\n",
      " -0.00951643 -0.0154359   0.00684894 -0.00169267  0.01858863  0.01781514\n",
      "  0.03903748 -0.00860734  0.01325879  0.04136097 -0.02193067 -0.00601569\n",
      " -0.01238216 -0.0058813  -0.02295544  0.00534021 -0.01738581  0.02630238\n",
      " -0.00871712 -0.01202903  0.00247699 -0.02098098 -0.01918662  0.01971563\n",
      " -0.03228301  0.01876739 -0.01522098  0.05103733  0.0549451   0.01249996\n",
      " -0.01154868  0.01629774  0.04219291 -0.03197624 -0.02136712  0.05144106\n",
      "  0.03577213  0.00876246 -0.04208024  0.02489294  0.01037199 -0.00275044\n",
      "  0.03379783  0.04465161 -0.02965501 -0.01369505 -0.00024853  0.02036837\n",
      "  0.02031791 -0.01623425 -0.0208155   0.00695388 -0.00746441 -0.01698858\n",
      "  0.00937977 -0.05166871 -0.02626767  0.01472135 -0.02318017 -0.03116026\n",
      " -0.01146997  0.01102895  0.02296478  0.0473918  -0.0064969   0.00450832\n",
      "  0.01058618 -0.00368076 -0.00738638 -0.02152404 -0.05724094 -0.03444711\n",
      "  0.01010136 -0.02229496 -0.01567211  0.00462068 -0.03427705  0.00909371\n",
      " -0.0469549   0.05386848 -0.02238842  0.00388095  0.03382432  0.04180418\n",
      " -0.00523124  0.01236022  0.04435254 -0.02665597 -0.04074226 -0.00075274\n",
      "  0.01727193 -0.01505976  0.00977161  0.00348338 -0.01895484 -0.03662665\n",
      " -0.01416912  0.01747706  0.03173725  0.01038962 -0.00213621  0.03082675\n",
      "  0.0132647   0.02020615 -0.0150073   0.01443153 -0.04142222 -0.02465193\n",
      " -0.01694023 -0.02270943  0.0067122  -0.01662881 -0.05859393  0.06325382\n",
      "  0.02558813 -0.01312799 -0.00690408 -0.01654302 -0.03021144  0.02272707\n",
      "  0.03601176  0.0053637   0.02954067 -0.03844069  0.0008339  -0.03405691\n",
      " -0.04838504 -0.01219167  0.02410567  0.00711239 -0.03046529  0.01623808\n",
      "  0.02385569  0.00777089 -0.02292665 -0.01698305 -0.00632172  0.01086698\n",
      " -0.01022527 -0.01167283 -0.02226297  0.03768409 -0.00052915 -0.00500513\n",
      " -0.02332587 -0.04317043  0.0106133  -0.01266192 -0.00314731  0.0227463\n",
      "  0.00450564  0.0214313  -0.03577463 -0.01770247  0.02870355  0.00550534\n",
      " -0.01904975 -0.00108495 -0.03003661  0.01144282 -0.03925749 -0.02916767\n",
      " -0.01895279  0.03083522 -0.02869373  0.04791095 -0.02988662  0.00363693\n",
      "  0.00505067 -0.01283402 -0.00196441 -0.00034245  0.00077759  0.05551191\n",
      " -0.01955539 -0.00615734  0.03365875 -0.02535384 -0.02226966 -0.01167864\n",
      "  0.02915299  0.00602596 -0.00278014  0.03446065  0.00836536 -0.03223\n",
      " -0.03893729  0.01883122  0.03123307 -0.0129948   0.01277874  0.00620419\n",
      "  0.02600997  0.02868619 -0.00763141  0.01512571 -0.0122053  -0.01766614\n",
      "  0.01645124 -0.01770219 -0.01966367 -0.00586252  0.02316632 -0.0256091\n",
      " -0.01029058 -0.01582804  0.00988616 -0.03693164 -0.01490673  0.01410069\n",
      "  0.03786719 -0.02170516  0.00259955 -0.01734877 -0.02736063 -0.00816315\n",
      "  0.05826313  0.01047709 -0.015224    0.00470132 -0.02790937  0.00742374\n",
      " -0.03995489  0.00076107 -0.03359183 -0.02576539  0.02139273 -0.00655188\n",
      " -0.00117271 -0.00235489  0.02145131 -0.04729421  0.01855836 -0.01219536\n",
      "  0.01180039 -0.034969    0.00256521  0.00733443 -0.02343219 -0.01203002\n",
      " -0.06702008  0.02417697 -0.01854866 -0.0249074   0.02095589 -0.00492049\n",
      " -0.04835116 -0.01980179 -0.0267825   0.00568123 -0.02006541  0.02990938\n",
      " -0.02087668 -0.01338215  0.00226975  0.03735065 -0.00325773  0.02028959\n",
      "  0.00443508 -0.01136111 -0.01870076 -0.00595286 -0.00107675 -0.01639683\n",
      "  0.03264728 -0.0369469  -0.0002705  -0.00979823  0.0104507   0.021433\n",
      " -0.04967578  0.01414886  0.03002779  0.0162484   0.0234209  -0.00937098\n",
      "  0.03759299 -0.01658621  0.00538885 -0.0290712  -0.01679389 -0.0150097\n",
      "  0.03840088  0.00236579 -0.03029044 -0.01919579 -0.02800312  0.0105692\n",
      "  0.00444419 -0.00439128 -0.00397838  0.00537011  0.01581595  0.01127488\n",
      " -0.0388902   0.01446622 -0.04659563 -0.03769409 -0.00419097 -0.03402374\n",
      " -0.02394229 -0.00885947 -0.02240919  0.04558223 -0.01185592  0.00172739\n",
      "  0.01252831  0.02605503 -0.00278791  0.02461595 -0.02204071 -0.02462635\n",
      "  0.00848422  0.0164479   0.01929212 -0.04518072  0.03009453 -0.01392477\n",
      " -0.00312705  0.00602074 -0.00740153  0.01397183 -0.01914447 -0.0114003\n",
      "  0.00039183 -0.00520957 -0.0214563  -0.0190566   0.01275321  0.01893798\n",
      " -0.0347501  -0.01147085  0.01055347 -0.02065559  0.02142642 -0.01588377\n",
      "  0.06495183  0.05309868 -0.03326124 -0.0212654   0.00473676  0.02660859\n",
      "  0.04003093  0.04103035 -0.02184959 -0.00557201 -0.02148831  0.0083989\n",
      "  0.01055313 -0.0155501  -0.00796855  0.00617691 -0.00532185 -0.014495\n",
      "  0.0207977  -0.01930208  0.01168566 -0.00927117 -0.01334242  0.0614092\n",
      " -0.01871972 -0.0256433   0.03617009 -0.03265081 -0.0478817   0.01905087\n",
      " -0.00749229 -0.03397783 -0.02464818 -0.02758096 -0.02407685 -0.04455054\n",
      " -0.0220185  -0.00338234 -0.00578653 -0.02372963 -0.01492884 -0.00320207\n",
      " -0.01053552 -0.00330415 -0.00903437  0.00858511 -0.02914346  0.06340905\n",
      " -0.01057691 -0.00400128  0.03361044 -0.01769577 -0.01731576 -0.00529834\n",
      "  0.049965   -0.02052691 -0.00889659 -0.00308645 -0.00535314  0.02312775\n",
      " -0.01434636 -0.00668843 -0.01232913  0.015277   -0.01645333 -0.00109932\n",
      " -0.01573445 -0.03073197  0.02172721  0.0057793  -0.00975346 -0.02484199\n",
      "  0.01331681 -0.00374524  0.02613567 -0.00572427 -0.03578341 -0.01828993\n",
      "  0.00529603 -0.03420635 -0.03008637 -0.06685948  0.03584353 -0.00793956\n",
      "  0.03073215 -0.02064406 -0.07145775  0.01204561  0.00062998 -0.02354739\n",
      "  0.04879313 -0.01774575 -0.01690639 -0.0051537   0.03725448  0.01777037\n",
      "  0.03315799  0.00550949  0.00352156  0.02591783 -0.04032523 -0.01468444\n",
      "  0.02702316 -0.02920365 -0.02195352  0.00933257 -0.06600633 -0.04702795\n",
      "  0.01670706  0.00708851  0.00173447 -0.06486755 -0.0246626  -0.00500074\n",
      " -0.00944618  0.0086163 ]\n"
     ]
    }
   ],
   "source": [
    "weights, bias = model.layers[0].get_weights()\n",
    "print(f\"Final weights: {weights}\")\n",
    "print(f\"Final bias: {bias}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
